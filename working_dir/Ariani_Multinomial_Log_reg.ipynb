{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2222f391-3ab5-4036-b313-21797263f66f",
   "metadata": {},
   "source": [
    "Logistic Regression (3-class vs 2-class) Apply Normalization on Dataset Use this as a baseline.\n",
    "\n",
    "### TODO:\n",
    "1) Test on cleaned data (did uncleaned)\n",
    "2) Test without all predictors (only necessary ones)\n",
    "3) Apply other testing statistics:\n",
    "      train time\n",
    "      AIC -> low is good\n",
    "      Confusion Matrix\n",
    "      ROC curve later (read up on it)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a388d18a-e3cd-461a-a3f9-9de05bedf307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('diabetes_012_health_indicators_BRFSS2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6cd044-2d62-4862-bd0d-c4ee279483be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original shape: (253680, 22)\n",
      "Cleaned shape: (144834, 22)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify binary and continuous columns\n",
    "binary_cols = [col for col in df.columns if sorted(df[col].dropna().unique()) == [0, 1]]\n",
    "continuous_cols = [col for col in df.columns if col not in binary_cols]\n",
    "# Step 2: Calculate Q1, Q2, Q3, IQR only for continuous columns\n",
    "Q1 = df[continuous_cols].quantile(0.25)\n",
    "Q2 = df[continuous_cols].quantile(0.50)  # Median\n",
    "Q3 = df[continuous_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Step 3: Remove outliers for continuous columns only\n",
    "# (Binary columns are untouched)\n",
    "filter_condition = ~((df[continuous_cols] < (Q1 - IQR)) | (df[continuous_cols] > (Q3 + IQR))).any(axis=1)\n",
    "df_clean = df[filter_condition]\n",
    "\n",
    "print(\"\\nOriginal shape:\", df.shape)\n",
    "print(\"Cleaned shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22da198c-3678-413f-9c83-1986bd268470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 9. 4. 3.]\n",
      " [0. 0. 0. ... 7. 6. 1.]\n",
      " [1. 1. 1. ... 9. 4. 8.]\n",
      " ...\n",
      " [0. 0. 1. ... 2. 5. 2.]\n",
      " [1. 0. 1. ... 7. 5. 1.]\n",
      " [1. 1. 1. ... 9. 6. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# pre data cleaned model simple benchmark classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "y = list(df[\"Diabetes_012\"])\n",
    "\n",
    "# omit target\n",
    "X = df.to_numpy()\n",
    "X = np.delete(X, 0,1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e829a-11dd-4cf6-9a38-a211693b25e1",
   "metadata": {},
   "source": [
    "Standardize All Features\n",
    "Transform data, both categorical and continuous to be on same scale.. mean of 0 and standard deviation of 1... btwn [-1, 1]\n",
    "Aka z-score normalization\n",
    "Create Test and Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd283d35-fec5-4b36-b2fe-57a3cfb09c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:1:2 proportions in the test dataset\n",
      "0.8432355723746452\n",
      "0.01859035004730369\n",
      "0.13817407757805109\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "standardize = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "X_train = standardize.fit_transform(X_train)\n",
    "X_test = standardize.fit_transform(X_test)\n",
    "\n",
    "print(\"0:1:2 proportions in the test dataset\")\n",
    "print(y_test.count(0)/len(y_test)) #no diabetes\n",
    "print(y_test.count(1)/len(y_test)) #prediabets\n",
    "print(y_test.count(2)/len(y_test)) #diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c6d4d-3856-4e66-9ffb-c8e5e6a6ab78",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression Model (Multinomial)\n",
    "- Including all 3 classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56d4f6f6-5bc2-4007-8267-d41dc5e394a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (unequal class weight) accuracy: 0.847839798170924\n"
     ]
    }
   ],
   "source": [
    "#Multi-class Regression with:\n",
    "#lbfgs solver... Quasi-Newton Method\n",
    "#class_weight = 'balanced'.. all classes treated equally\n",
    "\n",
    "logistic_mulitnomial = LogisticRegression(\n",
    "                    penalty='l2',\n",
    "                    fit_intercept=True,\n",
    "                    class_weight=None, # default... change for class imbalance\n",
    "                    solver='lbfgs',\n",
    "                   #multi_class = 'auto' -> function defaults to multinomial\n",
    "                    max_iter=100,\n",
    "                    random_state=0,\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_mulitnomial.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression (unequal class weight) accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669d082-463d-4714-b1c8-5fed724d1f7c",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "We can see that accuracy is roughly 85%, even with imbalanced class weighting. From our other model statistics, we know this to be the case because the model is almost always predicting no_diabetes (unequal weights). Therefore, this outcome doesn't actually prove that our model is performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f12a41-110e-4c29-a811-53964d272906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.847839798170924\n"
     ]
    }
   ],
   "source": [
    "logistic_mulitnomial = LogisticRegression(\n",
    "                    penalty='l2',\n",
    "                    fit_intercept=True,\n",
    "                    class_weight=None,\n",
    "                    solver='saga',\n",
    "                   #multi_class = 'auto' -> function defaults to multinomial\n",
    "                    max_iter=100,\n",
    "                    random_state=0,\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_mulitnomial.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9399e2-858a-46a3-b93f-20cd28cf3eef",
   "metadata": {},
   "source": [
    "Solvers 'lbfgs', 'newton-cg', 'sag', 'saga' all converge to same solution for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f803620e-74ea-47cf-acf7-7a671158a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (lbfgs) accuracy: 0.6465152948596657\n"
     ]
    }
   ],
   "source": [
    "#Multi-class regression with:\n",
    "#lbfgs solver... Quasi-Newton Method\n",
    "#class_weight = 'balanced'.. all classes treated equally\n",
    "\n",
    "logistic_mulitnomial = LogisticRegression(\n",
    "                    penalty='l2',\n",
    "                    fit_intercept=True,\n",
    "                    class_weight='balanced',\n",
    "                    solver='lbfgs',\n",
    "                   #multi_class = 'auto' -> function defaults to multinomial\n",
    "                    max_iter=100,\n",
    "                    random_state=0,\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_mulitnomial.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression (lbfgs) accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df05cf-f040-4b46-8b87-2194f791db26",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "As one can see, the accuracy dropped significantly compared to an unqual class weighting. In reality, this may not be bad, because it shows that the model is making an effort to predict class 1 or 2, not just class 0. Calculated below are the class weights, which illustrate a significant penalization for incorrectly classifying group_2 compared to no_diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810943ad-cba4-414d-8b81-975607e0828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed class weights (balanced): {0.0: 0.3958183804025589, 2.0: 2.3857352443290827, 1.0: 18.371958285052145}\n"
     ]
    }
   ],
   "source": [
    "# Weights for balanced classes\n",
    "# Model is heavily penalized for incorrect group_2 classifications\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Assume y_train is your training label array\n",
    "class_counts = Counter(y_train)\n",
    "n_samples = len(y_train)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "weights = {cls: n_samples / (n_classes * count) for cls, count in class_counts.items()}\n",
    "\n",
    "print(\"Computed class weights (balanced):\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ea66c18-c06a-42d3-9eea-40411fc9ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (newton-cg) accuracy: 0.6463418479974772\n",
      "Logistic Regression (sag) accuracy: 0.5932986439608956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acts5\\miniconda3\\envs\\cenv4py310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Weights for balanced classes\n",
    "# solver = newton-cg\n",
    "\n",
    "logistic_mulitnomial = LogisticRegression(\n",
    "                    class_weight='balanced',\n",
    "                    solver='newton-cg',\n",
    "                    random_state=0,\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_mulitnomial.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression (newton-cg) accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "logistic_mulitnomial = LogisticRegression(\n",
    "                    class_weight='balanced',\n",
    "                    solver='saga',\n",
    "                    max_iter=100,\n",
    "                    random_state=0,\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_mulitnomial.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression (sag) accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d185a9-be9c-4576-b651-ddaec152c72d",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "Multinomial Logistic Regression (lbfgs) accuracy is about 65%, same for Newton-CG loss minimizer, while the Sag solver didn't converge, so for 100 iterations it's about 59%. Trying the saga solver, it also didn't converge and after 1000 iterations, only had about 53% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
