{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf6d458-8454-4f46-9919-5d66a0bbcc15",
   "metadata": {},
   "source": [
    "Logistic Regression (3-class vs 2-class)\n",
    "Apply Normalization on Dataset\n",
    "Use this as a baseline.\n",
    "\n",
    "Different:\n",
    "1) optimization techniques\n",
    "2) regularization (L2 vs L1 vs none) aka penalty\n",
    "3) cleaned vs uncleaned data\n",
    "4) all predictors vs only uncorrelated ones\n",
    "Measure:\n",
    "1) accuracy (correct/total)\n",
    "2) train time\n",
    "3) AIC -> low is good\n",
    "4) Confusion Matrix\n",
    "5) ROC curve later (read up on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5735296-4238-4dcd-b658-c204dd96963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 5406643..5949bf3\n",
      "Fast-forward\n",
      " .gitignore          |    6 +-\n",
      " ljmurphy.ipynb      | 2238 --------------------------------------------\n",
      " murphy-5050-v.ipynb | 2552 +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " murphy-base-v.ipynb |  122 +++\n",
      " 4 files changed, 2678 insertions(+), 2240 deletions(-)\n",
      " delete mode 100644 ljmurphy.ipynb\n",
      " create mode 100644 murphy-5050-v.ipynb\n",
      " create mode 100644 murphy-base-v.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/LJMurphyy/diabetes-risk-predictor\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   5406643..5949bf3  main       -> origin/main\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15454585-4b55-4820-a52b-23051051a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('diabetes_binary_5050split_health_indicators_BRFSS2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d18958-d070-4d60-8b08-61b45707a033",
   "metadata": {},
   "source": [
    "## Remove Outliers for Continuous Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b699aeca-efb1-4e53-88df-b387e1bcaf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original shape: (253680, 22)\n",
      "Cleaned shape: (144834, 22)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify binary and continuous columns\n",
    "binary_cols = [col for col in df.columns if sorted(df[col].dropna().unique()) == [0, 1]]\n",
    "continuous_cols = [col for col in df.columns if col not in binary_cols]\n",
    "# Step 2: Calculate Q1, Q2, Q3, IQR only for continuous columns\n",
    "Q1 = df[continuous_cols].quantile(0.25)\n",
    "Q2 = df[continuous_cols].quantile(0.50)  # Median\n",
    "Q3 = df[continuous_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Step 3: Remove outliers for continuous columns only\n",
    "# (Binary columns are untouched)\n",
    "filter_condition = ~((df[continuous_cols] < (Q1 - IQR)) | (df[continuous_cols] > (Q3 + IQR))).any(axis=1)\n",
    "df_clean = df[filter_condition]\n",
    "\n",
    "print(\"\\nOriginal shape:\", df.shape)\n",
    "print(\"Cleaned shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08631f4-56ee-4ebc-b01e-ca2a83946b65",
   "metadata": {},
   "source": [
    "### Combine classes to binary classification \n",
    "- Create numpy array containing all 21 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf519167-6055-49de-b416-3d1c12fdf14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8424116997792495\n",
      "0.018255282245348472\n",
      "0.13933301797540207\n",
      "[[1. 1. 1. ... 9. 4. 3.]\n",
      " [0. 0. 0. ... 7. 6. 1.]\n",
      " [1. 1. 1. ... 9. 4. 8.]\n",
      " ...\n",
      " [0. 0. 1. ... 2. 5. 2.]\n",
      " [1. 0. 1. ... 7. 5. 1.]\n",
      " [1. 1. 1. ... 9. 6. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# proportions of 0:1:2 diabetes\n",
    "y = list(df[\"Diabetes_012\"])\n",
    "length = len(y)\n",
    "print(y.count(0)/length) # 0 = no diabetes\n",
    "print(y.count(1)/length) # 1 = pre-diabetes\n",
    "print(y.count(2)/length) # 2 = diabetes\n",
    "\n",
    "\n",
    "# pre data cleaned model simple benchmark classification\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "y = list(df[\"Diabetes_012\"])\n",
    "y = list(map(lambda X: 1 if X == 2 or X == 1 else X, y))\n",
    "\n",
    "# omit target\n",
    "X = df.to_numpy()\n",
    "X = np.delete(X, 0,1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7de5e-0756-4dda-ad36-e6b9e8a69b94",
   "metadata": {},
   "source": [
    "### Standardize All Features\n",
    "- Transform data, both categorical and continuous to be on same scale.. mean of 0 and standard deviation of 1... btwn [-1, 1]\n",
    "- Aka z-score normalization\n",
    "- Create Test and Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ac76d88-4a51-43ff-89ff-ee097e349839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:1 proportions in the test dataset\n",
      "0.8432355723746452\n",
      "0.15676442762535478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "standardize = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "X_train = standardize.fit_transform(X_train)\n",
    "X_test = standardize.fit_transform(X_test)\n",
    "\n",
    "print(\"0:1 proportions in the test dataset\")\n",
    "print(y_test.count(0)/len(y_test))\n",
    "print(y_test.count(1)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae14108-bc2c-457c-bf52-2be73d407bb5",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression Model (Multinomial)\n",
    "- Including all 3 classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108ca0a4-5fba-4b5f-a133-a88bdde4f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logistic_baseline = LogisticRegression(\n",
    "                    penalty='l2',\n",
    "                    dual=False,\n",
    "                    tol=0.0001,\n",
    "                    C=1.0,\n",
    "                    fit_intercept=True,\n",
    "                    intercept_scaling=1,\n",
    "                    class_weight=None,\n",
    "                    solver='lbfgs',\n",
    "                    max_iter=100,\n",
    "                    multi_class='auto',\n",
    "                    random_state=None,\n",
    "                    n_jobs=None,\n",
    "                    l1_ratio=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7abf10-54ef-437d-97ae-3cec9d54672a",
   "metadata": {},
   "source": [
    "### Binomial Logistic Regression\n",
    "- One that Varsha did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e17309-ca8b-465f-a09e-ff6b8190a738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acts5\\miniconda3\\envs\\cenv4py310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.8489120151371807\n"
     ]
    }
   ],
   "source": [
    "logistic_binomial = LogisticRegression(\n",
    "                    penalty='l2',\n",
    "                    class_weight=None,\n",
    "                    solver='lbfgs',\n",
    "                    max_iter=100,\n",
    "                    multi_class='auto',\n",
    "                    random_state=0,\n",
    "                    n_jobs=None,\n",
    "                    l1_ratio=None).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_binomial.predict(X_test)\n",
    "\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
